{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credits: https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = ['In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next genaeration of students with AI-ready skills. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. Thatâ€™s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = ['''Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems. The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.\n",
    "After the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.\n",
    "Once we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.\n",
    "The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters. After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems. The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.\\nAfter the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.\\nOnce we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.\\nThe following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters. After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems.',\n",
       " 'The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.',\n",
       " 'After the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.',\n",
       " 'Once we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.',\n",
       " 'The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters.',\n",
       " 'After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = sent_tokenize(para[0])\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = tf.saved_model.load(export_dir=r\"C:\\Users\\bishw\\OneDrive\\Hackathon\\Shop 101\\use\\128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed = []\n",
    "for i in sent:\n",
    "    word_embed.append(embed([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = []\n",
    "for i in range(0,len(word_embed)):\n",
    "    temp = []\n",
    "    for j in range(0, len(word_embed)):\n",
    "        if sent[i] != sent[j]:\n",
    "            temp.append(np.absolute(tf.keras.losses.cosine_similarity(word_embed[i], word_embed[j]).numpy())[0])\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    similarity_matrix.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a adjacency matrix?\n",
    "# Explain page rank algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.5550171, 0.49654168, 0.4363834, 0.48939455, 0.4501098],\n",
       " [0.5550171, 0, 0.84216905, 0.8174871, 0.7248665, 0.65586674],\n",
       " [0.49654168, 0.84216905, 0, 0.84389377, 0.75197625, 0.68663204],\n",
       " [0.4363834, 0.8174871, 0.84389377, 0, 0.63960624, 0.7225522],\n",
       " [0.48939455, 0.7248665, 0.75197625, 0.63960624, 0, 0.6302664],\n",
       " [0.4501098, 0.65586674, 0.68663204, 0.7225522, 0.6302664, 0]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.matrix(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = nx.pagerank(sentence_similarity_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.13021853027415753,\n",
       " 1: 0.1822498993543313,\n",
       " 2: 0.18330567277945828,\n",
       " 3: 0.1759915661289379,\n",
       " 4: 0.16616041487142433,\n",
       " 5: 0.1620739165916905}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.18330567277945828,\n",
       "  'After the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.'),\n",
       " (0.1822498993543313,\n",
       "  'The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.'),\n",
       " (0.1759915661289379,\n",
       "  'Once we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.'),\n",
       " (0.16616041487142433,\n",
       "  'The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters.'),\n",
       " (0.1620739165916905,\n",
       "  'After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.'),\n",
       " (0.13021853027415753,\n",
       "  'Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems.')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sent)), reverse=True)    \n",
    "ranked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_text = []\n",
    "for i in range(5):\n",
    "      summarize_text.append(ranked_sentence[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.',\n",
       " 'The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.',\n",
       " 'Once we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.',\n",
       " 'The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters.',\n",
       " 'After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import re'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import re'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import re'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', para[0] )\n",
    "formatted_article_text = re.sub(r'\\s+', ' ', para[0])\n",
    "formatted_article_text = re.sub(r'[()]', ' ', para[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems. The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.\\nAfter the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.\\nOnce we have computed this aggregated error  known as cost function , we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.\\nThe following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters. After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = nltk.sent_tokenize(formatted_article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_freq = {}\n",
    "for word in nltk.word_tokenize(formatted_article_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_freq.keys():\n",
    "            word_freq[word] = 1\n",
    "        else:\n",
    "            word_freq[word] = word_freq[word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequncy = max(word_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_freq.keys():\n",
    "    word_freq[word] = (word_freq[word]/maximum_frequncy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_freq.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_freq[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_freq[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems.': 2.375,\n",
       " 'The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters.': 5.625}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters. Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems.\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "summary_sentences = heapq.nlargest(2, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import tensorflow as tf\n",
    "import tensorflow as hub\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import warnings\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "if os.path.exists(r\"D:\\B2B_Git_Instance\\sentence_summarizer/use/512/\") != True:\n",
    "    print(\"downloading USE(512 dim) model\")\n",
    "    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "    useLarge = os.path.join(r\"D:\\B2B_Git_Instance\\sentence_summarizer\", \"use/512/\")\n",
    "    tf.saved_model.save(embed, useLarge)\n",
    "else:\n",
    "    print(\"Already exists\")\n",
    "    \n",
    "class SentenceSummarizer:\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        self.embed = tf.saved_model.load(export_dir=r\"D:\\B2B_Git_Instance\\sentence_summarizer\\use\\512\")\n",
    "\n",
    "    def embedder(self):\n",
    "        sent = sent_tokenize(self.sentence)\n",
    "        word_embed = []\n",
    "        for i in sent:\n",
    "            word_embed.append(self.embed([i]))\n",
    "        return word_embed, sent\n",
    "\n",
    "    def similarityMatrix(self,word_embed,sentence):\n",
    "        self.word_embed = word_embed\n",
    "        self.sentence = sentence\n",
    "        similarity_matrix = []\n",
    "        for i in range(0, len(self.word_embed)):\n",
    "            temp = []\n",
    "            for j in range(0, len(self.word_embed)):\n",
    "                if self.sentence[i] != self.sentence[j]:\n",
    "                    temp.append(\n",
    "                        np.absolute(tf.keras.losses.cosine_similarity(self.word_embed[i], self.word_embed[j]).numpy())[\n",
    "                            0])\n",
    "                else:\n",
    "                    temp.append(0)\n",
    "            similarity_matrix.append(temp)\n",
    "        similarity_matrix = np.matrix(similarity_matrix)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def sentRank(self,similarity_matrix,sentence):\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "        self.sentence = sentence\n",
    "        sentence_similarity_graph = nx.from_numpy_array(self.similarity_matrix)\n",
    "        scores = nx.pagerank(sentence_similarity_graph)\n",
    "        ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(self.sentence)), reverse=True)\n",
    "        return ranked_sentence\n",
    "\n",
    "    def sentSummarize(self,ranked_sentence):\n",
    "        self.ranked_sentence = ranked_sentence\n",
    "        lines = 5\n",
    "#         lines = int(\n",
    "#             input(\"How many line summary you want? Has to be less than the number of sentence in the original para\"))\n",
    "        summarize_text = []\n",
    "        for i in range(lines):\n",
    "            summarize_text.append(self.ranked_sentence[i][1])\n",
    "            \n",
    "        return ''.join(summarize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = 'what can be gleaned from the decline in cases nationwide is that covid restrictions like no indoor dining and widespread mask wearing work. as andy slavitt the former head of the centers for medicare and medicaid wrote this week recent declines like arizonas test positive ratio dropping from to are largely credited to the fact that people started behaving. this is a good sign as it shows that the united states knows what policies lead to tangible changes in disease spread and saved lives. as tom frieden md a former director of the u.s. centers for disease control and prevention cdc shared on monday the trends are real but precarious national test positivity decreased from . to . which confirms that there has been a steady decrease in cases over the past few weeks. case rates in the northeast remain relatively low and these states are now joined by mi wv nm mt wy ak which have relatively low rates. the number of tests done decreased in some states including florida. but as frieden notes these declining case loads are still high. there remains concern that resurgences will happen especially as the school year kicks off with many students having inperson class. already there are reports of outbreaks among college students around the united states.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what can be gleaned from the decline in cases nationwide is that covid restrictions like no indoor dining and widespread mask wearing work. as andy slavitt the former head of the centers for medicare and medicaid wrote this week recent declines like arizonas test positive ratio dropping from to are largely credited to the fact that people started behaving. this is a good sign as it shows that the united states knows what policies lead to tangible changes in disease spread and saved lives. as tom frieden md a former director of the u.s. centers for disease control and prevention cdc shared on monday the trends are real but precarious national test positivity decreased from . to . which confirms that there has been a steady decrease in cases over the past few weeks. case rates in the northeast remain relatively low and these states are now joined by mi wv nm mt wy ak which have relatively low rates. the number of tests done decreased in some states including florida. but as frieden notes these declining case loads are still high. there remains concern that resurgences will happen especially as the school year kicks off with many students having inperson class. already there are reports of outbreaks among college students around the united states.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 30 calls to <function recreate_function.<locals>.restored_function_body at 0x000002C0D0B39A68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 30 calls to <function recreate_function.<locals>.restored_function_body at 0x000002C0D0B39A68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many line summary you want? Has to be less than the number of sentence in the original para6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i think we are seeing a decline in cases in certain parts of the country that were prior hot spots says amesh adalja md a senior scholar at the johns hopkins university center for health security.cases of the coronavirus in the united states increased significantly in june and july before plateauing and then finally dropping  a trend thats being seen currently though there continue to be new emerging hot spots in areas like the midwest.while the u.s. case numbers are likely an underestimate across the board experts say the decrease in cases cannot be attributed to testing failures as there are similar drops in hospitalizations and a lower share of positive test results the new york times reports.but you have to remember that were still not testing enough so that whatever number of cases you see is still in underestimate.what can be gleaned from the decline in cases nationwide is that covid restrictions like no indoor dining and widespread mask wearing work.the latest data shows that cases of covid in the united states have been dropping  though they still remain at high levels  and experts say its important to embrace some of the learnings of the past couple months and prepare for the fall.many of the states with the biggest drops in cases also had the biggest outbreaks in july like florida and texas.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = SentenceSummarizer(''.join(df['cleaned_content'][14:18].values))\n",
    "# a = SentenceSummarizer(para)\n",
    "vec, sent = a.embedder()\n",
    "sim_mat = a.similarityMatrix(word_embed=vec, sentence=sent)\n",
    "rank = a.sentRank(similarity_matrix=sim_mat, sentence=sent)\n",
    "a.sentSummarize(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\B2B_Git_Instance\\sentence_summarizer\\queryTAsk-om3query-202108821.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['id','cleaned_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what can be gleaned from the decline in cases nationwide is that covid restrictions like no indoor dining and widespread mask wearing work. as andy slavitt the former head of the centers for medicare and medicaid wrote this week recent declines like arizonas test positive ratio dropping from  to  are largely credited to the fact that people started behaving. this is a good sign as it shows that the united states knows what policies lead to tangible changes in disease spread and saved lives.  as tom frieden md a former director of the u.s. centers for disease control and prevention cdc shared on monday the trends are real but precarious national test positivity decreased from . to . which confirms that there has been a steady decrease in cases over the past few weeks. case rates in the northeast remain relatively low and these states are now joined by mi wv nm mt wy ak which have relatively low rates. the number of tests done decreased in some states including florida. but as frieden notes these declining case loads are still high. there remains concern that resurgences will happen especially as the school year kicks off with many students having inperson class. already there are reports of outbreaks among college students around the united states.written by  the issue will be how long these trends remain in place as reopening plans continue. especially now that there are going to be larger gatherings of people indoors and as the nation heads into cooler months where outdoor activity in many states becomes more difficult. the pattern seems to be that areas get in trouble and then targeted public health actions are put into place and they get some semblance of control adalja says. this is the pattern that i think will repeat in different parts of the country over and over again. read more written by the latest news expert advice and analysis to keep you safe plus personal stories of life in a pandemictake a lookthe latest news expert advice and analysis to keep you safe plus personal stories of life in a pandemictake a look  as thomas smith writes in elemental a supercomputer at oak ridge national lab in tennessee crunched data on more than  genes from  genetic samples in an effort to better understand covid and has revealed a new theory for how covid affects the body called the bradykinin hypothesis. its one potential explanation for some of covids bizarre symptoms and may also provide insights into possible treatments. read about it below.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(df['cleaned_content'][15:18].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\B2B_Git_Instance\\sentence_summarizer\\use/512/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\B2B_Git_Instance\\sentence_summarizer\\use/512/assets\n"
     ]
    }
   ],
   "source": [
    "# downloading USE(512 dim) model\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "useLarge = os.path.join(r\"D:\\B2B_Git_Instance\\sentence_summarizer\", \"use/512/\")\n",
    "tf.saved_model.save(embed, useLarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
