{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = ['In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. Thatâ€™s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = ['''Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems. The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.\n",
    "After the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.\n",
    "Once we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.\n",
    "The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters. After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems. The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.\\nAfter the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.\\nOnce we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.\\nThe following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters. After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems.',\n",
       " 'The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.',\n",
       " 'After the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.',\n",
       " 'Once we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.',\n",
       " 'The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters.',\n",
       " 'After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = sent_tokenize(para[0])\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = tf.saved_model.load(export_dir=r\"C:\\Users\\bishw\\OneDrive\\Hackathon\\Shop 101\\use\\128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed = []\n",
    "for i in sent:\n",
    "    word_embed.append(embed([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = []\n",
    "for i in range(0,len(word_embed)):\n",
    "    temp = []\n",
    "    for j in range(0, len(word_embed)):\n",
    "        if sent[i] != sent[j]:\n",
    "            temp.append(np.absolute(tf.keras.losses.cosine_similarity(word_embed[i], word_embed[j]).numpy())[0])\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    similarity_matrix.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a adjacency matrix?\n",
    "# Explain page rank algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.5550171, 0.49654168, 0.4363834, 0.48939455, 0.4501098],\n",
       " [0.5550171, 0, 0.84216905, 0.8174871, 0.7248665, 0.65586674],\n",
       " [0.49654168, 0.84216905, 0, 0.84389377, 0.75197625, 0.68663204],\n",
       " [0.4363834, 0.8174871, 0.84389377, 0, 0.63960624, 0.7225522],\n",
       " [0.48939455, 0.7248665, 0.75197625, 0.63960624, 0, 0.6302664],\n",
       " [0.4501098, 0.65586674, 0.68663204, 0.7225522, 0.6302664, 0]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.matrix(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = nx.pagerank(sentence_similarity_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.13021853027415753,\n",
       " 1: 0.1822498993543313,\n",
       " 2: 0.18330567277945828,\n",
       " 3: 0.1759915661289379,\n",
       " 4: 0.16616041487142433,\n",
       " 5: 0.1620739165916905}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.18330567277945828,\n",
       "  'After the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.'),\n",
       " (0.1822498993543313,\n",
       "  'The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.'),\n",
       " (0.1759915661289379,\n",
       "  'Once we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.'),\n",
       " (0.16616041487142433,\n",
       "  'The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters.'),\n",
       " (0.1620739165916905,\n",
       "  'After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.'),\n",
       " (0.13021853027415753,\n",
       "  'Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems.')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sent)), reverse=True)    \n",
    "ranked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_text = []\n",
    "for i in range(5):\n",
    "      summarize_text.append(ranked_sentence[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.',\n",
       " 'The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.',\n",
       " 'Once we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.',\n",
       " 'The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters.',\n",
       " 'After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import re'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import re'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import re'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', para[0] )\n",
    "formatted_article_text = re.sub(r'\\s+', ' ', para[0])\n",
    "formatted_article_text = re.sub(r'[()]', ' ', para[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems. The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.\\nAfter the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.\\nOnce we have computed this aggregated error  known as cost function , we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.\\nThe following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters. After one iteration of gradient descent, we move to the blue point which is directly right and down from the initial orange point: we have gone in the direction of descending gradient.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = nltk.sent_tokenize(formatted_article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_freq = {}\n",
    "for word in nltk.word_tokenize(formatted_article_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_freq.keys():\n",
    "            word_freq[word] = 1\n",
    "        else:\n",
    "            word_freq[word] = word_freq[word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequncy = max(word_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_freq.keys():\n",
    "    word_freq[word] = (word_freq[word]/maximum_frequncy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_freq.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_freq[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_freq[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems.': 2.375,\n",
       " 'The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters.': 5.625}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following figure shows graphically how this is done: we start at the orange point, which is the initial random value of the model parameters. Gradient Descent is an optimisation algorithm that can be used in a wide variety of problems.\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "summary_sentences = heapq.nlargest(2, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import tensorflow as tf\n",
    "import tensorflow as hub\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class SentenceSummarizer:\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        self.embed = tf.saved_model.load(export_dir=r\"C:\\Users\\bishw\\OneDrive\\Hackathon\\Shop 101\\use\\128\")\n",
    "\n",
    "    def embedder(self):\n",
    "        sent = sent_tokenize(self.sentence)\n",
    "        word_embed = []\n",
    "        for i in sent:\n",
    "            word_embed.append(self.embed([i]))\n",
    "        return word_embed, sent\n",
    "\n",
    "    def similarityMatrix(self,word_embed,sentence):\n",
    "        self.word_embed = word_embed\n",
    "        self.sentence = sentence\n",
    "        similarity_matrix = []\n",
    "        for i in range(0, len(self.word_embed)):\n",
    "            temp = []\n",
    "            for j in range(0, len(self.word_embed)):\n",
    "                if self.sentence[i] != self.sentence[j]:\n",
    "                    temp.append(\n",
    "                        np.absolute(tf.keras.losses.cosine_similarity(self.word_embed[i], self.word_embed[j]).numpy())[\n",
    "                            0])\n",
    "                else:\n",
    "                    temp.append(0)\n",
    "            similarity_matrix.append(temp)\n",
    "        similarity_matrix = np.matrix(similarity_matrix)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def sentRank(self,similarity_matrix,sentence):\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "        self.sentence = sentence\n",
    "        sentence_similarity_graph = nx.from_numpy_array(self.similarity_matrix)\n",
    "        scores = nx.pagerank(sentence_similarity_graph)\n",
    "        ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(self.sentence)), reverse=True)\n",
    "        return ranked_sentence\n",
    "\n",
    "    def sentSummarize(self,ranked_sentence):\n",
    "        self.ranked_sentence = ranked_sentence\n",
    "        lines = int(\n",
    "            input(\"How many line summary you want? Has to be less than the number of sentence in the original para\"))\n",
    "        summarize_text = []\n",
    "        for i in range(lines):\n",
    "            summarize_text.append(self.ranked_sentence[i][1])\n",
    "            \n",
    "        return ''.join(summarize_text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 16 calls to <function recreate_function.<locals>.restored_function_body at 0x000002AD0C8AACA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002AD11FB3280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002AD084C9A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "How many line summary you want? Has to be less than the number of sentence in the original para2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'After the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.The general idea of this method is to iteratively tweak the parameters of a model in order to reach the set of parameter values that minimises the error that such model makes in its predictions.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = SentenceSummarizer(para[0])\n",
    "vec, sent = a.embedder()\n",
    "sim_mat = a.similarityMatrix(word_embed=vec, sentence=sent)\n",
    "rank = a.sentRank(similarity_matrix=sim_mat, sentence=sent)\n",
    "a.sentSummarize(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
